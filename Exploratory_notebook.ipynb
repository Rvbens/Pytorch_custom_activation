{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.autograd import Function, gradcheck\n",
    "from torch.utils.cpp_extension import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extending Pytorch: https://pytorch.org/docs/master/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#myActFunc(x) = 0.5 x ( tanh(x) + 1 )\n",
    "class CustomAct(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        \n",
    "        ctx.save_for_backward(inp)\n",
    "        out = 0.5 * inp * ( torch.tanh(inp) + 1)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        inp = ctx.saved_tensors[0]\n",
    "        #0.5 tanh(x) + 0.5 x sech^2(x) + 0.5\n",
    "        grad_inp = 0.5 * torch.tanh(inp) + 0.5 * inp * (1/torch.cosh(inp))**2 + 0.5\n",
    "        return grad_inp * grad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "act = CustomAct.apply\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "inps = torch.randn(2,4,dtype=torch.double,requires_grad=True)\n",
    "test = gradcheck(act, inps, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom C++ and CUDA extensions: https://pytorch.org/tutorials/advanced/cpp_extension.html\n",
    "Tensor basics(C++): https://pytorch.org/cppdocs/notes/tensor_basics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cust_act_cpp\n",
    "cust_act_cpp = load(name='cust_act_cpp', sources=['cust_act.cpp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAct_c(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        \n",
    "        ctx.save_for_backward(inp)\n",
    "        out = cust_act_cpp.forward(inp)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        inp = ctx.saved_tensors[0]\n",
    "        #0.5 tanh(x) + 0.5 x sech^2(x) + 0.5\n",
    "        grad_inp = cust_act_cpp.backward(inp,grad_out)\n",
    "        return grad_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "act_c = CustomAct_c.apply\n",
    "inps = torch.randn(2,4,dtype=torch.double,requires_grad=True)\n",
    "test = gradcheck(act_c, inps, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom C++ and CUDA extensions: https://pytorch.org/tutorials/advanced/cpp_extension.html\n",
    "An even easier introduction to CUDA: https://devblogs.nvidia.com/even-easier-introduction-cuda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cust_act_cuda = load(name='cust_act', sources=['cust_act_cuda.cpp', 'cust_act_cuda_kernel.cu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAct_cuda(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        \n",
    "        ctx.save_for_backward(inp)\n",
    "        out = cust_act_cuda.forward(inp)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, d_out):\n",
    "        inp = ctx.saved_tensors[0]\n",
    "        #0.5 tanh(x) + 0.5 x sech^2(x) + 0.5\n",
    "        grad_inp = cust_act_cuda.backward(inp,d_out.contiguous())\n",
    "        return grad_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "act_cuda = CustomAct_cuda.apply\n",
    "inps = torch.randn(2,4,dtype=torch.double,requires_grad=True,device='cuda')\n",
    "test = gradcheck(act_cuda, inps, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(func,iters=10_000, device='cpu'):\n",
    "    act = func.apply\n",
    "    batch_size = 64\n",
    "    input_features = 1_000\n",
    "    X = torch.randn(batch_size, input_features,requires_grad=True,device=device)\n",
    "\n",
    "    forward = 0\n",
    "    backward = 0\n",
    "    for _ in range(iters):\n",
    "        start = time.time()\n",
    "        out = act(X)\n",
    "        forward += time.time() - start\n",
    "\n",
    "        start = time.time()\n",
    "        out.sum().backward()\n",
    "        backward += time.time() - start\n",
    "\n",
    "    print('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/iters, backward * 1e6/iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward: 276.037 us | Backward 711.382 us\n",
      "Forward: 129.084 us | Backward 367.477 us\n"
     ]
    }
   ],
   "source": [
    "#python\n",
    "profile(CustomAct)\n",
    "profile(CustomAct,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward: 196.571 us | Backward 734.201 us\n",
      "Forward: 123.962 us | Backward 343.428 us\n"
     ]
    }
   ],
   "source": [
    "#c++\n",
    "profile(CustomAct_c)\n",
    "profile(CustomAct_c,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward: 64.148 us | Backward 200.492 us\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "profile(CustomAct_cuda,device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
